# ğŸŒŸ ETL Pipelines Tutorial | World Bank Datasets ğŸŒŸ  ğŸ‘‰
 design, manage and optimize the flow of data with dynamic pipelines. This project uses data from the World Bank comes from two sources. you'll create a single Python module that creates a pipeline in these datasets, transforms them, and loads the results into the database all in dynamic pipelines. âœ¨ğŸ¯ğŸ’¥

_____
- **ETL Tools :** Extract Transform Load (ETL) is a category of technologies that move data between systems. These tools access data from many different technologies, and then apply rules to â€œtransformâ€ and cleanse the data so that it is ready for analysis.

**ETL stands for Extract, Transform, Load.**

This project uses data from the World Bank. The data comes from two sources:

[World Bank Indicator Data](https://data.worldbank.org/indicator) - This data contains socio-economic indicators for countries around the world. A few example indicators include population, arable land, and central government debt.

[World Bank Project Data](https://datacatalog.worldbank.org/dataset/world-bank-projects-operations) - This data set contains information about World Bank project lending since 1947.

#### Outline of this notebook:
**Extract data from different sources such as:**

- csv files
- json files
- APIs

**Transform data**

- combining data from different sources
- data cleaning
- data types
- parsing dates
- file encodings
- missing data
- duplicate data
- dummy variables
- remove outliers
- scaling features
- engineering features

**Load**

- send the transformed data to a database

**ETL Pipeline**

- code an ETL pipeline


The end goal is to clean these data sets and bring them together into one table. As you'll see, it's not as easy as one might hope. By the end of the notebook, I'll have written an ETL pipeline to extract, transform, and load this data into a new database.

The goal of the notebook is to combine these data sets together so that you can run a machine learning model predicting World Bank Project total costs.

In the process, you'll need to transform these data sets in different ways. And finally, you'll write a single Python module that creates a pipeline in these datasets, transforms them, and loads the results into the database all in one step.
____

## ğŸ¯ DATA ENGINEERING : Concept & Importance ğŸ¯

![0](https://user-images.githubusercontent.com/44786324/205437950-77059d40-dbaf-4d7b-9580-276f2a140394.png)
